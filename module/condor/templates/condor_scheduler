##  With flocking we need to let the SCHEDD trust the other
##  negotiators we are flocking with as well.  You should normally
##  not have to change this either.
HOSTALLOW_NEGOTIATOR_SCHEDD = $(HOSTALLOW_NEGOTIATOR_SCHEDD), $(CONDOR_HOST), $(FLOCK_NEGOTIATOR_HOSTS)
HOSTALLOW_WRITE = $(HOSTALLOW_WRITE), <%= sched_writes %>

##--------------------------------------------------------------------
##  Settings that control the daemon's debugging output:
##--------------------------------------------------------------------
MAX_SCHEDD_LOG          = 1000000
SCHEDD_DEBUG            = D_PID

MAX_SHADOW_LOG          = 1000000
SHADOW_DEBUG            =

######################################################################
##  Daemon-wide settings:
######################################################################
##  Log files
SCHEDD_LOG      = $(LOG)/SchedLog
SHADOW_LOG      = $(LOG)/ShadowLog

##  Lock files
SHADOW_LOCK     = $(LOCK)/ShadowLock

######################################################################
##  Daemon-specific settings:
######################################################################
# Where is the binary located?
SCHEDD                          = $(SBIN)/condor_schedd

##  Daemons you want the master to keep running for you:
DAEMON_LIST = $(DAEMON_LIST), SCHEDD

##--------------------------------------------------------------------
##  condor_schedd
##--------------------------------------------------------------------
##  Where are the various shadow binaries installed?
SHADOW_LIST = SHADOW, SHADOW_STANDARD
SHADOW                  = $(SBIN)/condor_shadow
SHADOW_STANDARD         = $(SBIN)/condor_shadow.std

##  When the schedd starts up, it can place it's address (IP and port)
##  into a file.  This way, tools running on the local machine don't
##  need to query the central manager to find the schedd.  This
##  feature can be turned off by commenting out this setting.
SCHEDD_ADDRESS_FILE     = $(LOG)/.schedd_address

##  Additionally, a daemon may store its ClassAd on the local filesystem
##  as well as sending it to the collector. This way, tools that need
##  information about a daemon do not have to contact the central manager
##  to get information about a daemon on the same machine.
##  This feature is necessary for Quill to work.
SCHEDD_DAEMON_AD_FILE = $(LOG)/.schedd_classad

##  How often should the schedd send an update to the central manager?
#SCHEDD_INTERVAL        = 300

##  How long should the schedd wait between spawning each shadow?
#JOB_START_DELAY        = 2

##  How many concurrent sub-processes should the schedd spawn to handle
##  queries?  (Unix only)
#SCHEDD_QUERY_WORKERS   = 3

##  How often should the schedd send a keep alive message to any
##  startds it has claimed?  (5 minutes)
#ALIVE_INTERVAL         = 300

##  This setting controls the maximum number of times that a
##  condor_shadow processes can have a fatal error (exception) before
##  the condor_schedd will simply relinquish the match associated with
##  the dying shadow.
#MAX_SHADOW_EXCEPTIONS  = 5

##  Estimated virtual memory size of each condor_shadow process.
##  Specified in kilobytes.
SHADOW_SIZE_ESTIMATE    = 1800

##  The condor_schedd can renice the condor_shadow processes on your
##  submit machines.  How "nice" do you want the shadows? (1-19).
##  The higher the number, the lower priority the shadows have.
# SHADOW_RENICE_INCREMENT       = 0

## The condor_schedd can renice scheduler universe processes
## (e.g. DAGMan) on your submit machines.  How "nice" do you want the
## scheduler universe processes? (1-19).  The higher the number, the
## lower priority the processes have.
# SCHED_UNIV_RENICE_INCREMENT = 0

##  By default, when the schedd fails to start an idle job, it will
##  not try to start any other idle jobs in the same cluster during
##  that negotiation cycle.  This makes negotiation much more
##  efficient for large job clusters.  However, in some cases other
##  jobs in the cluster can be started even though an earlier job
##  can't.  For example, the jobs' requirements may differ, because of
##  different disk space, memory, or operating system requirements.
##  Or, machines may be willing to run only some jobs in the cluster,
##  because their requirements reference the jobs' virtual memory size
##  or other attribute.  Setting NEGOTIATE_ALL_JOBS_IN_CLUSTER to True
##  will force the schedd to try to start all idle jobs in each
##  negotiation cycle.  This will make negotiation cycles last longer,
##  but it will ensure that all jobs that can be started will be
##  started.
#NEGOTIATE_ALL_JOBS_IN_CLUSTER = False

## This setting controls how often, in seconds, the schedd considers
## periodic job actions given by the user in the submit file.
## (Currently, these are periodic_hold, periodic_release, and periodic_remove.)
#PERIODIC_EXPR_INTERVAL = 60

######
## Queue management settings:
######
##  How often should the schedd truncate it's job queue transaction
##  log?  (Specified in seconds, once a day is the default.)
#QUEUE_CLEAN_INTERVAL   = 86400

##  How often should the schedd commit "wall clock" run time for jobs
##  to the queue, so run time statistics remain accurate when the 
##  schedd crashes?  (Specified in seconds, once per hour is the
##  default.  Set to 0 to disable.) 
#WALL_CLOCK_CKPT_INTERVAL = 3600

##  What users do you want to grant super user access to this job
##  queue?  (These users will be able to remove other user's jobs).
##  By default, this only includes root.
QUEUE_SUPER_USERS       = root, condor 

##--------------------------------------------------------------------
##  condor_shadow
##--------------------------------------------------------------------
##  If the shadow is unable to read a checkpoint file from the
##  checkpoint server, it keeps trying only if the job has accumulated
##  more than MAX_DISCARDED_RUN_TIME seconds of CPU usage.  Otherwise,
##  the job is started from scratch.  Defaults to 1 hour.  This
##  setting is only used if USE_CKPT_SERVER (from above) is True.
#MAX_DISCARDED_RUN_TIME = 3600

##  Should periodic checkpoints be compressed?
#COMPRESS_PERIODIC_CKPT = False

##  Should vacate checkpoints be compressed?
#COMPRESS_VACATE_CKPT = False

##  Should we commit the application's dirty memory pages to swap
##  space during a periodic checkpoint?
#PERIODIC_MEMORY_SYNC = False

##  Should we write vacate checkpoints slowly?  If nonzero, this
##  parameter specifies the speed at which vacate checkpoints should
##  be written, in kilobytes per second.
#SLOW_CKPT_SPEED = 0

##  How often should the shadow update the job queue with job
##  attributes that periodically change?  Specified in seconds.
#SHADOW_QUEUE_UPDATE_INTERVAL = 15 * 60

##  Should the shadow wait to update certain job attributes for the
##  next periodic update, or should it immediately these update
##  attributes as they change?  Due to performance concerns of
##  aggressive updates to a busy condor_schedd, the default is True.
#SHADOW_LAZY_QUEUE_UPDATE = TRUE

#####################################################################
##  This where you choose the configuration that you would like to
##  use.  It has no defaults so it must be defined.
######################################################################
##  When should a local universe job be allowed to start?
START_LOCAL_UNIVERSE    = True
# Only start a local universe jobs if there are less
# than 100 local jobs currently running
#START_LOCAL_UNIVERSE   = TotalLocalJobsRunning < 100

##  When should a scheduler universe job be allowed to start?
START_SCHEDULER_UNIVERSE        = True
# Only start a scheduler universe jobs if there are less
# than 100 scheduler jobs currently running
#START_SCHEDULER_UNIVERSE       = TotalSchedulerJobsRunning < 100

